{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SelmaDM/DesignPattern/blob/master/CNN-Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky2Cxj4aSc2I"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xghBy3Z0OWmn"
      },
      "source": [
        "### Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKHZkH7oSWSQ"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import requests\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L7l34WWMsif"
      },
      "source": [
        "Installation de la bibliothèque **librosa** qui lit les fichiers audio et extrait les spectrogrammes qui seront traités ensuite comme des images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PczEBkMMr3e"
      },
      "source": [
        "!pip -q install librosa\n",
        "import librosa\n",
        "import librosa.display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8at1lVnUEdy"
      },
      "source": [
        "### Chargement du corpus\n",
        "\n",
        "Le corpus est constitué de 10 concepts audio qui sont :\n",
        "* **tronçonneuse** *(chainsaw)*\n",
        "* **tic-tac d'une horloge** *(clock_tick)*\n",
        "* **craquement de feu** *(crackling_fire)*\n",
        "* **pleurs de bébé** *(crying_baby)*\n",
        "* **chien** *(dog)*\n",
        "* **hélicoptère** *(helicopter)*\n",
        "* **pluie** *(rain)*\n",
        "* **coq** *(rooster)*\n",
        "* **bruit des vagues** *(sea_waves)*\n",
        "* **éternuement** *(sneezing)*\n",
        "\n",
        "Nous voulons classer les différents fichiers audio suivant ces 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNWQ51sMX8bI"
      },
      "source": [
        "#### Téléchargement du corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c91NX1YSUiMv"
      },
      "source": [
        "!wget -O dataset.zip https://www.irit.fr/~Thomas.Pellegrini/ens/M2RFA/dataset.zip\n",
        "!unzip -qq dataset.zip -d data/\n",
        "!rm dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYNvYdf4X09N"
      },
      "source": [
        "La base de données téléchargée est stockée dans :\n",
        "\n",
        "```\n",
        "./data\n",
        "```\n",
        "Vous pouvez visualiser l'arborescence des données dans la colonne de gauche sous l'onglet *Fichiers* et en cliquant sur le bouton *actualiser* si les données n'apparaissent pas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnjO0mDqYDBH"
      },
      "source": [
        "base_dir = './data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03n0UdLTQOrz"
      },
      "source": [
        "#### Chargement du corpus dans la mémoire\n",
        "\n",
        "La fonction **load_dataset** permet de charger les données d'apprentissage ou les données de test en mémoire.\n",
        "\n",
        "**Attention :** le chargement est un peu long (environ 2min)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4l0zPxX-C-E"
      },
      "source": [
        "idx_to_classes = ['chainsaw', 'clock_tick', 'crackling_fire', 'crying_baby',\n",
        "                  'dog', 'helicopter', 'rain', 'rooster', 'sea_waves',\n",
        "                  'sneezing']\n",
        "\n",
        "\n",
        "def load_dataset(path):\n",
        "    spec = []\n",
        "    labels = []\n",
        "    for idx, label in enumerate(idx_to_classes):\n",
        "        with os.scandir(os.path.join(path, label)) as it:\n",
        "            for entry in it:\n",
        "                if (not entry.name.startswith('.')\n",
        "                        and entry.name.endswith('.wav')):\n",
        "                    # load audio\n",
        "                    y, sr = librosa.load(entry.path)\n",
        "                    # convert audio to melspectrogram\n",
        "                    spec.append(librosa.core.power_to_db(librosa.feature.melspectrogram(y=y, sr=sr,\n",
        "                                                              n_fft=2048,\n",
        "                                                              hop_length=512,\n",
        "                                                              power=2.0)))\n",
        "                    \n",
        "                    # add associated label\n",
        "                    labels.append(idx)\n",
        "\n",
        "    # mélange les données \n",
        "    #combined = list(zip(spec, labels))\n",
        "    #random.shuffle(combined)\n",
        "    #spec, labels = zip(*combined)\n",
        "    return np.array(spec), np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8cYfqtgp9NF"
      },
      "source": [
        "%%time\n",
        "\n",
        "print(\"Chargement du corpus d'apprentissage\")\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "x_train, y_train = load_dataset(train_dir)\n",
        "\n",
        "print(\"Chargement du corpus de test\")\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "x_test, y_test = load_dataset(test_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT-qxqPCYSsz"
      },
      "source": [
        "#### Taille du corpus\n",
        "\n",
        "Affichage de la taille du corpus. Pour cela, il suffit de compter le nombre d'images qu'il y a dans les dossiers correspondant à chaque label aussi bien dans le corpus d'apprentissage que dans le corpus de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii-3-soLq4S5"
      },
      "source": [
        "# Affichage du nombre d'exemples totales dans le corpus\n",
        "print('Taille du corpus total')\n",
        "print('\\t• train :', len(x_train), 'exemples')\n",
        "print('\\t• test :', len(x_test), 'exemples')\n",
        "\n",
        "# Affichage de la taille des images et des labels dans le corpus \n",
        "print('\\nTaille des données d\\'apprentissage')\n",
        "print('\\t• X_train (images) :', x_train.shape)\n",
        "print('\\t• y_train (labels) :', y_train.shape)\n",
        "\n",
        "print('\\nTaille des données de test')\n",
        "print('\\t• X_test (images) :', x_test.shape)\n",
        "print('\\t• y_test (labels) :', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QFyxg4LQb3X"
      },
      "source": [
        "**Correspondance ID du label / signification**\n",
        "\n",
        "Définition de la liste *idx_to_classes* permettant à partir de la valeur du label de retrouver sa signification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFv4XAmiQdFj"
      },
      "source": [
        "idx_to_classes = ['chainsaw', 'clock_tick', 'crackling_fire', 'crying_baby',\n",
        "                  'dog', 'helicopter', 'rain', 'rooster', 'sea_waves',\n",
        "                  'sneezing']\n",
        "\n",
        "# du corpus d'apprentissage\n",
        "print(\"Affichage de la correspondance des labels :\")\n",
        "for i in range(10):\n",
        "    n = random.randint(0, len(y_train)-1)\n",
        "    print('• y_train[' + str(n) + '] =', y_train[n], '->', idx_to_classes[y_train[n]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_ENMhcyMM8T"
      },
      "source": [
        "**Affichage de quelques spectrogrammes**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i84bFM7lyeQs"
      },
      "source": [
        "plt.figure(figsize=(13,20))\n",
        "\n",
        "n = 0\n",
        "for i in range(10):\n",
        "    for j in range(5):\n",
        "        # récupération d'une image et de son label associé\n",
        "        img, target = x_train[n+j], y_train[n+j]\n",
        "        # affiche du spectrogramme\n",
        "        plt.subplot(10,5,i*5+j+1)\n",
        "        # img = librosa.power_to_db(img)\n",
        "        librosa.display.specshow(img, cmap='magma')\n",
        "        # ajout d'un titre à l'image\n",
        "        plt.title('{} ({})'.format(idx_to_classes[target], target))\n",
        "        #plt.colorbar(format='%+2.0f dB')\n",
        "    n += 32\n",
        "              \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGkAIzzL7cxt"
      },
      "source": [
        "# Générateurs de données pour Pytorch\n",
        "\n",
        "Création d'un générateur de données (un « DataLoader ») pour les jeux de train et de test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPyDjezr7iZu"
      },
      "source": [
        "nb_classes = 10\n",
        "\n",
        "# numpy vers tensors\n",
        "y_train = torch.from_numpy(y_train)\n",
        "y_test = torch.from_numpy(y_test)\n",
        "\n",
        "\n",
        "height = x_train[0].shape[0]\n",
        "width = x_train[0].shape[1]\n",
        "print(height, width)\n",
        "\n",
        "# numpy vers tensors\n",
        "X_train = torch.from_numpy(x_train)\n",
        "X_test = torch.from_numpy(x_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMzSkmos7wy1"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, x_data, y_labels):\n",
        "        'Initialization'\n",
        "        self.y = y_labels\n",
        "        self.x = x_data\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.x)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        X = self.x[index].unsqueeze_(0)\n",
        "        y = self.y[index]\n",
        "\n",
        "        return X, y\n",
        "\n",
        "dataset_train = Dataset(X_train, y_train)\n",
        "dataset_test = Dataset(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O8K7Mic7y9d"
      },
      "source": [
        "batch_size=32\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TzyAT93Yvqj"
      },
      "source": [
        "## Classification de fichiers audio en utilisant de l'apprentissage profond\n",
        "\n",
        "Maintenant c'est à vous de jouer, vous devez implémenter différentes architectures de réseaux de neurones à partir de vos connaissances et ceux que vous avez vu dans les séances précédentes.\n",
        "\n",
        "Pour cela, vous devez :\n",
        "* complèter l'architecture MLP puis celle du CNN plus bas;\n",
        "* lancer l'apprentissage ;\n",
        "* évaluer les modèles sur le jeu de test ;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud8WRg599utN"
      },
      "source": [
        "### Réseau MLP\n",
        "\n",
        "Dans cette section, il faut compléter le réseau dense ci-dessous, qui contient une couche cachée avec la fonction d'activation ReLU et une couche de sortie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdhOZgSayUo-"
      },
      "source": [
        "# Perceptron multi-couche \n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, num_hidden=50):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(??, num_hidden)\n",
        "        self.layer2 = nn.Linear(??, ??)\n",
        "        # vous pouvez définir d'autres couches dans un deuxième temps\n",
        "\n",
        "    def forward(self, spectro):\n",
        "        flattened = spectro.view(-1, ??) # flatten le spectro\n",
        "        ???\n",
        "        return ??\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-AWcXC5JdaK"
      },
      "source": [
        "Compléter la fonction d'apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y895rmi10Dsk"
      },
      "source": [
        "def train(model, batch_size=32, weight_decay=0.0,\n",
        "          optimizer=\"sgd\", learning_rate=0.1, momentum=0.9, \n",
        "          num_epochs=10):\n",
        "    \n",
        "    # la loss \n",
        "    criterion = nn.??\n",
        "    # l'optimiseur\n",
        "    assert optimizer in (\"sgd\", \"adam\")\n",
        "    if optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(model.parameters(),\n",
        "                              lr=learning_rate,\n",
        "                              momentum=momentum,\n",
        "                              weight_decay=weight_decay)\n",
        "    else:\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                               lr=learning_rate,\n",
        "                               weight_decay=weight_decay)\n",
        "    # on track la learning curve avec des listes\n",
        "    iters, iters_acc, losses, train_acc, val_acc = [], [], [], [], []\n",
        "    # training\n",
        "    n = 0 # nombre d'iterations (pour faire des figures)\n",
        "    for epoch in range(num_epochs):\n",
        "        for imgs, labels in iter(train_loader):\n",
        "            # if imgs.size()[0] < batch_size:\n",
        "            #     continue\n",
        "            # print(imgs.size())\n",
        "\n",
        "            model.train() # met le modèle en mode train\n",
        "            out = model(??)\n",
        "            loss = criterion(??)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # sauvegarde iteration et loss\n",
        "            iters.append(n)\n",
        "            losses.append(float(loss)/batch_size)             # loss moyen\n",
        "\n",
        "            if n % 20 == 0 :\n",
        "              train_acc.append(get_accuracy(model, train=True)) # training accuracy \n",
        "              val_acc.append(get_accuracy(model, train=False))  # test accuracy\n",
        "              iters_acc.append(n)\n",
        "\n",
        "            n += 1\n",
        "\n",
        "    # plotting\n",
        "    plt.title(\"Courbe d'apprentissage\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Courbe d'apprentissage\")\n",
        "    plt.plot(iters_acc, train_acc, label=\"Train\")\n",
        "    plt.plot(iters_acc, val_acc, label=\"Test\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Training Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Précision finale Train : {}\".format(train_acc[-1]))\n",
        "    print(\"Précision finale Test : {}\".format(val_acc[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlFejAPD0Wdz"
      },
      "source": [
        "def get_accuracy(model, train=False):\n",
        "    if train:\n",
        "        data = train_loader\n",
        "    else:\n",
        "        data = test_loader\n",
        "\n",
        "    model.eval() # met le modèle en mode test (inhibe le dropout par exemple)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inp, labels in data:\n",
        "        output = model(inp) # pas besoin de torch.softmax\n",
        "        pred = output.max(1, keepdim=True)[1] # retrouve l'indice de la log-proba maximale\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += inp.shape[0]\n",
        "    return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtwMMG_6OkCo"
      },
      "source": [
        "Instancier un MLP avec 50 neurones pour la couche cachée. Quel est le nombre de paramètres de ce modèle ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44q7bQlS5_Nr"
      },
      "source": [
        "model = ??\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qz3SZO3OouW"
      },
      "source": [
        "Tester un apprentissage de 10 epochs, avec l'optimiseur ADAM, un taux d'apprentissage à 0.0001, une taille de batch de 32 exemples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIiB1kys4R6P"
      },
      "source": [
        "train(??)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdx_fGg2OKR4"
      },
      "source": [
        "On voit que le MLP n'est pas performant. Vous pourrez durant le temps que vous avez pour préparer le rapport essayer de modifier son architecture pour tenter d'améliorer ses performances.\n",
        "\n",
        "\n",
        "Passons à un réseau convolutif, un CNN. \n",
        "\n",
        "Compléter son implémentation ci-dessous pour définir trois couches de convolution avec des noyaux de taille 3x3 et à 8, 16 et 32 canaux en sortie respectivement. \n",
        "\n",
        "Compléter les dimensions qui vont bien pour les deux couches fully-connected fc1 et fc2. fc1 a 50 neurones et fc2 10 neurones pour les 10 classes. \n",
        "\n",
        "Une ReLU sera appliquée après chaque couche de convolution et après fc1. \n",
        "\n",
        "Le pooling s'applique après chaque couche de convolution.\n",
        "\n",
        "Compléter la méthode forward. Vous pouvez afficher les dimensions de la sortie de la dernière couche de convolution, après le pooling, pour déterminer la dimension que vous devez règler pour la couche fc1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-G2R9t78qQ7"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = ??\n",
        "        self.conv2 = ??\n",
        "        self.conv3 = ??\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        self.fc1 = nn.Linear(??, ??)\n",
        "        self.fc2 = nn.Linear(??, ??)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = ???\n",
        "\n",
        "\n",
        "        x = x.view(-1, ??) # flatten\n",
        "        x = ??\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vKeUV_uOP7K"
      },
      "source": [
        "Instancier le modèle CNN et afficher son nombre de paramètres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95q4bp5I8qBG"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FBGjCmPOWnP"
      },
      "source": [
        "Tester un apprentissage de 10 epochs, avec l'optimiseur ADAM, un taux d'apprentissage à 0.0001, une taille de batch de 32 exemples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sSVJIzG0Ybn"
      },
      "source": [
        "train(??)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "216L6XHSObM3"
      },
      "source": [
        "Essayer d'améliorer le MLP, jouer avec son architecture : changer la fonction d'activation, modifier le nombre de couches, ajouter une normalisation des activations...\n",
        "\n",
        "Même chose avec le CNN. \n",
        "\n",
        "Vous pouvez améliorer les hyperparamètres d'apprentissage de la fonction train.\n",
        "\n",
        "Essayer les deux optimiseurs proposés. L'un est-il meilleur que l'autre pour la tâche donnée ? Pensez-vous avoir une bonne valeur de taux d'apprentissage pour SGD (et pour ADAM) ?\n",
        "\n",
        "Une fois que vous avez votre meilleur modèle, faites une matrice de confusion qui montre les pourcentages de confusion entre les 10 classes sur le jeu de test.\n",
        "\n",
        "Pour aller plus loin : quelles augmentations pourraient êter pertinentes pour augmenter les spectrogrammes ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRuDID9R2Afy"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}